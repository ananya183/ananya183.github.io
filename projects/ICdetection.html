<html>
  <head>
	  	<link rel="shortcut icon" type="image/png" href="./icon.png" />
		<title>Automatic Identification of Brain Independent Components in Electroencephalography Data Collected while Standing in a Virtually Immersive Environment - A Deep Learning-Based Approach</title>
		 <SCRIPT SRC='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></SCRIPT>
		<SCRIPT>MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}})</SCRIPT>
</head>

  <body>
    <br>
          <center>
          	<span style="font-size:38px">Automatic Identification of Brain Independent <br> Components in Electroencephalography Data <br> Collected while Standing in a Virtually Immersive <br> Environment - A Deep Learning-Based Approach <br></span>
			  <br>
	  		  <table align=center width=700px>
	  			  <tr>
					  <td align=center>
						  <span style="font-size:25px">
							  <a href="https://kaurrachneet6.github.io/">Rachneet Kaur</a>,
							  <a href="https://www.linkedin.com/in/maxim-korolkov-04004bb5/">Maxim Korolkov</a>,
							  <br> <a href="https://ahs.illinois.edu/hernandez">Manuel Hernandez</a>,
							  <a href="https://ise.illinois.edu/directory/profile/r-sowers">Richard Sowers</a>
							  
						  </span>
					  </td>
				  </tr>
				  <tr>
					  <td align="center">
						  <span style="font-size:22px">
							  University of Illinois at Urbana-Champaign
						  </span>
						  <br>
						  <span style="font-size:25px">
							  <a href="https://ieeexplore.ieee.org/document/9035020">IEEE EMBC 2020</a>
						  </span>
						  <br>
						  <br>
					  </td>
				  </tr>
			  </table>

	  		  <table align=center width=600px>
	  			  <tr>
	  	              <td align=center width=140px>
	  					<center>
	  						<span style="font-size:22px"><a href='https://github.com/kaurrachneet6' target="_blank"> [Code]</a></span>
		  		  		</center>
		  		  	  </td>
	  	              
	  	              <td align=center width=150px>
	  					<center>
	  						<span style="font-size:22px"><a href='./ICdetectionEMBC2020_slides.pdf' target="_blank"> [Slides]</a></span>
		  		  		</center>
		  		  	  </td>
 
<!-- 		  		  	  <td align=center width=200px>
	  					<center>
	  						<span style="font-size:22px"><a href='https://youtu.be/EnYwCX5E9gk' target="_blank"> [Teaser Video]</a></span>
		  		  		</center>
		  		  	  </td> -->
					  
					  <td align=center width=200px>
	  					<center>
	  						<span style="font-size:22px"><a href='https://www.youtube.com/watch?v=ei4Xs8M8a-w&feature=emb_logo&ab_channel=HernandezLab' target="_blank"> [Video]</a></span>
		  		  		</center>
		  		  	  </td>

	  	              <td align=center width=100px>
	  					<center>
	  						<span style="font-size:22px"><a href='https://ieeexplore.ieee.org/document/9175741' target="_blank"> [Paper]</a></span>
		  		  		</center>
		  		  	  </td>

		  		  	 </tr>
	  			  <tr>
			  </table>
          </center>
	  <br>
		<table align=center width=800px>
				<tr><td><center><h1>Abstract</h1></center></td></tr>  
			  <tr>
				  <td style="font-size:15pt; text-align:justify">
					Electroencephalography (EEG) is a commonly used method for monitoring brain activity. Automating an EEG signal processing pipeline is imperative to the exploration of real-time brain computer interface (BCI) applications. EEG analysis demands substantial training and time for removal of distinct unwanted independent components (ICs), generated via independent component analysis, corresponding to artifacts. The considerable subject-wise variations across these components motivates defining a procedural way to identify and eliminate these artifacts. We propose DeepIC-virtual, a convolutional neural network (CNN) deep learning classifier to automatically identify brain components in the ICs extracted from the subject's EEG data gathered while they are being immersed in a virtual reality (VR) environment. This work examined the feasibility of DL techniques to provide automated ICs classification on noisy and visually engaging upright stance EEG data. We collected the EEG data for six subjects while they were standing upright in a VR testing setup simulating pseudo-randomized variations in height and depth conditions and induced perturbations. An extensive 1432 IC representation images data set was generated and manually labelled via an expert as brain components or one of the six distinct removable artifacts. The supervised CNN architecture was utilized to categorize good brain ICs and bad artifactual ICs via generated images of topographical maps. Our model categorizing good versus bad IC topographical maps resulted in a binary classification accuracy and area under curve of 89.20% and 0.93 respectively. Despite significant imbalance, only 1 out of the 57 present brain ICs in the withheld testing set was miss-classified as an artifact. These results will hopefully encourage clinicians to integrate BCI methods and neurofeedback to control anxiety and provide a treatment of acrophobia, given the viability of automatic classification of artifactual ICs.
				  </td>
			  </tr>
	  </table>

          <table align=center width=850px>
  			  <tr>
				  <td>
					  <br>
					  <center>
							<img class="round" width="480" height="500" src="./ICdetection_image2.JPG"/>
					  
				  <div style='width: 700px; text-align: center;'> 
					  <b>Figure:</b> EEG data collection setup: The training and testing data for this study was accumulated from a designed system with real-time EEG recording setup while a subject stands quietly to pseudo-randomized height and depth alterations and induced perturbations.
				</div>
				  </center>	  
				<br>
				  </td>
			  </tr>
	</table>
          <table align=center width=850px>
  			  <tr>
				  <td>
					  <br>
					  <center>
							<img class="round" width="570" height="350" src="./ICdetection_image1.JPG"/>
					  
				  <div style='width: 700px; text-align: center;'> 
					  <b>Figure:</b> Artifactual IC rejection. <b>Left:</b> An accepted independent component. The IC representation consists of a scalp topography image (top left), an event-related potential (ERP) image (top right), a time series displaying activity from the component (middle right, below the ERP diagram) and an activity power spectrum plot (bottom). In the scalp projections diagram, red, blue and white colors denote positive, negative and null contributions respectively. The heat map demonstrates inputs from multiple electrodes. <b>Right:</b> A rejected independent component representing a muscle artifact from the side of head. The power spectrum plot reflects no apparent peaks in the alpha-band.
				</div>
				  </center>	  
				<br>
				  </td>
			  </tr>
	</table>
<!-- 			  <tr>
				  <td>
					  <br>
					  <center>
							<img class="round" width="900px" src="./FloorPlan229_physics__15__video.gif"/>
					  </center>
					  <br>
				  </td>
			  </tr> -->
  		  </table>

          <hr>

<!--   		  <table align=center width=850px>
	  		  <tr><td><center><h1>SYNC-Policies</h1></center></td></tr>
			  <tr>
	  		  	<td style="font-size:15pt; text-align:justify">
					Previous multi-agent RL methods consist of a communicative backbone followed by a single marginal policy per agent. This limits the joint policy that these methods can capture. The joint policy is a rank one matrix, particularly, the outer product of the marginal policies of the two agents. For the general case of <i>N</i> agents, this is a rank one tensor.
					<span style="font-size:28px">
						$$\Pi_{\text{marginal}} = \pi^1\otimes\cdots \otimes \pi^N$$
					</span>
					We leverage the communication band to move beyond this rank one joint policy. SYNC policies weave
					<i>m</i> marginal policies per agent using synchronized sampling (details in the paper). This leads to a more expressive joint policy as a mixture of marginal policies. For the <i>N</i> agent setup, the joint policy becomes:
					<span style="font-size:28px">
						$$\Pi_{\text{SYNC}} = \sum_{j=1}^m \alpha_j \cdot \pi^1_{j} \otimes \ldots \otimes \pi^N_{j}$$
					</span>
	  		    </td>
	  		  </tr>
			  <tr>
				  <td>
					  <center>
							<img class="round" width="600px" src="./sync_policies.png"/>
					  </center>
				  </td>
			  </tr>
			</table> -->
<!--   		  <br> -->
<!-- 		  <hr> -->
			<table align=center width=800px>
				<tr><td><center><h1>Explanatory video</h1></center></td></tr>
				<tr>
					<td style="font-size:15pt; text-align:justify">
						<center>
							A detailed video, corresponding slides: <a href='./ICdetectionEMBC2020_slides.pdf' target="_blank"> [Detailed slides]</a>.
						</center>
					</td>
				</tr>
				<tr>
					<td>
						<center>
							<iframe width="560" height="315" src="https://www.youtube.com/embed/ei4Xs8M8a-w" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
						</center>
					</td>
				</tr>
			</table>
			<br>
			<hr>
<!-- 			<table align=center width=800px>
				<tr><td><center><h1>Qualitative Results</h1></center></td></tr>
				<tr>
					<td style="font-size:15pt; text-align:justify">
						<span>
							This video highlighting policy roll-outs of marginal policies vs. SYNC-policies. Additionally, we include a way to <i>experience</i> what the agents 'tell' each other (enable audio for this clip).
						</span>
					</td>
				</tr>
				<tr>
					<td>
						<center>
							<iframe width="560" height="315" src="https://www.youtube.com/embed/I_Evs5Bol6k" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
						</center>
					</td>
				</tr>
			</table>
			<br>
			<hr> -->



			<table align=center width=800px>
				<tr><td><center><h1>References</h1></center></td></tr>
				<tr>
					<td style="font-size:15pt; text-align:justify">
						<span>
							Rachneet Kaur, Maxim Korolkov, Manuel E Hernandez, Richard Sowers. <b>Automatic Identification of Brain Independent Components in Electroencephalography Data Collected while Standing in a Virtually Immersive Environment - A Deep Learning-Based Approach</b>. In <i>IEEE EMBC</i> 2020 <a href="../Data/DeepIC_EMBC2020.bib" target="_blank" style="font-size:15pt;">[Bibtex]</a>
						</span>
					</td>
				</tr>
				<tr>
					<td>
						<center>
							<a href="https://ieeexplore.ieee.org/document/9175741">
								<img class="round" width= "200" src="./DeepIC_EMBC2020_thumbnail.png"/>
							</a>
						</center>
					</td>
				</tr>
			</table>

  		  <br>
		  <hr>

			<table align=center width=900px>
				<tr><td><center><h1>Acknowledgements</h1></center></td></tr>
				<tr>
					<td style="font-size:15pt; text-align:justify">
						<span>
							We would like to thank all of the subjects who participated in this study. This work would not have been possible without the financial support of JUMP ARCHES. We would also like to thank members of the <a href="http://mfp.kch.illinois.edu/">Mobility and Fall Prevention Lab</a> for their contributions to the study, and the Campus Research Board for financial support of this work. RK is thankful to William Chittenden for the William A. Chittenden Award.
						</span>
					</td>
				</tr>
			</table>

		<hr>
        <table style="font-size:14px" align=center>
        <tr>
        <td>
            Website adapted from <a href="https://deanplayerljx.github.io/tabvcr/">Jingxiang</a>, <a href="https://richzhang.github.io/colorization">Richard</a> and <a href="https://pathak22.github.io/modular-assemblies/">Deepak</a>.
        </td>
        </tr>
        </table>
              
</body>
</html>
